{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf03a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2 \n",
    "import tensorflow\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5d9d9",
   "metadata": {},
   "source": [
    "Dividing the data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea35f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data\\train\\benign\n",
      "Created: data\\train\\malignant\n",
      "Created: data\\val\\benign\n",
      "Created: data\\val\\malignant\n",
      "Created: data\\test\\benign\n",
      "Created: data\\test\\malignant\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory not found: pic\\benign. Please check your SOURCE_DIR configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m create_directory_structure()\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Split and copy files\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43msplit_and_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Verify results\u001b[39;00m\n\u001b[32m     81\u001b[39m verify_splits()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36msplit_and_copy\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Verify source directory exists\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(class_path):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please check your SOURCE_DIR configuration.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Get list of files for current class\u001b[39;00m\n\u001b[32m     29\u001b[39m files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(class_path) \n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(os.path.join(class_path, f))]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory not found: pic\\benign. Please check your SOURCE_DIR configuration."
     ]
    }
   ],
   "source": [
    "#I divided it before so i can't divide  itagain\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Updated Configuration - CHANGE THESE PATHS!\n",
    "SOURCE_DIR = 'pic'  # Changed from 'raw_data' to 'pic' based on your error\n",
    "DEST_DIR = 'data'\n",
    "CLASSES = ['benign', 'malignant']\n",
    "SPLIT_RATIOS = (0.7, 0.15, 0.15)  # Train, Val, Test\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create empty directory structure\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for cls in CLASSES:\n",
    "            path = os.path.join(DEST_DIR, split, cls)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(f'Created: {path}')\n",
    "\n",
    "def split_and_copy():\n",
    "    \"\"\"Split data and copy to destination directories\"\"\"\n",
    "    for cls in CLASSES:\n",
    "        class_path = os.path.join(SOURCE_DIR, cls)\n",
    "        \n",
    "        # Verify source directory exists\n",
    "        if not os.path.exists(class_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {class_path}. Please check your SOURCE_DIR configuration.\")\n",
    "            \n",
    "        # Get list of files for current class\n",
    "        files = [f for f in os.listdir(class_path) \n",
    "                if os.path.isfile(os.path.join(class_path, f))]\n",
    "        print(f'\\nProcessing {cls}: {len(files)} images in {class_path}')\n",
    "        \n",
    "        # Split into train, temp (val+test)\n",
    "        train_files, temp_files = train_test_split(\n",
    "            files,\n",
    "            test_size=SPLIT_RATIOS[1] + SPLIT_RATIOS[2],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Split temp into val and test\n",
    "        val_files, test_files = train_test_split(\n",
    "            temp_files,\n",
    "            test_size=SPLIT_RATIOS[2]/(SPLIT_RATIOS[1]+SPLIT_RATIOS[2]),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Copy files to destination\n",
    "        def copy_files(files, split):\n",
    "            src_dir = os.path.join(SOURCE_DIR, cls)\n",
    "            dst_dir = os.path.join(DEST_DIR, split, cls)\n",
    "            for f in files:\n",
    "                shutil.copy(os.path.join(src_dir, f), dst_dir)\n",
    "        \n",
    "        copy_files(train_files, 'train')\n",
    "        copy_files(val_files, 'val')\n",
    "        copy_files(test_files, 'test')\n",
    "        \n",
    "        print(f'Train: {len(train_files)}')\n",
    "        print(f'Val: {len(val_files)}')\n",
    "        print(f'Test: {len(test_files)}')\n",
    "\n",
    "def verify_splits():\n",
    "    \"\"\"Verify final distribution\"\"\"\n",
    "    print('\\nFinal verification:')\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        total = 0\n",
    "        for cls in CLASSES:\n",
    "            count = len(os.listdir(os.path.join(DEST_DIR, split, cls)))\n",
    "            total += count\n",
    "            print(f'{split}/{cls}: {count}')\n",
    "        print(f'Total {split}: {total}\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create empty directory structure\n",
    "    create_directory_structure()\n",
    "    \n",
    "    # Split and copy files\n",
    "    split_and_copy()\n",
    "    \n",
    "    # Verify results\n",
    "    verify_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09903be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\train\\benign\n",
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\train\\malignant\n",
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\val\\benign\n",
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\val\\malignant\n",
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\test\\benign\n",
      "Created: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\test\\malignant\n",
      "ùóòùó•ùó•ùó¢ùó•: Missing directory C:\\Users\\hatem\\codes\\breast cancer detection for course project\\pic\\benign\n",
      "Current working directory: c:\\Users\\hatem\\codes\\breast cancer detection for course project\n",
      "Full path attempted: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\pic\\benign\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "SOURCE_DIR = r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\pic'\n",
    "DEST_DIR = r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data'\n",
    "CLASSES = ['benign', 'malignant']\n",
    "SPLIT_RATIOS = (0.7, 0.15, 0.15)  # Train, Val, Test\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create empty directory structure\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for cls in CLASSES:\n",
    "            path = os.path.join(DEST_DIR, split, cls)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(f'Created: {path}')\n",
    "\n",
    "def split_and_copy():\n",
    "    \"\"\"Split data and copy to destination directories\"\"\"\n",
    "    for cls in CLASSES:\n",
    "        class_path = os.path.join(SOURCE_DIR, cls)\n",
    "        \n",
    "        #\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"ùóòùó•ùó•ùó¢ùó•: Missing directory {class_path}\")\n",
    "            print(f\"Current working directory: {os.getcwd()}\")\n",
    "            print(\"Full path attempted:\", os.path.abspath(class_path))\n",
    "            raise SystemExit(1)\n",
    "            \n",
    "        # Get list of files\n",
    "        files = [f for f in os.listdir(class_path) \n",
    "                if os.path.isfile(os.path.join(class_path, f))]\n",
    "        print(f'\\nProcessing {cls}: {len(files)} images')\n",
    "        \n",
    "        # Split files\n",
    "        train_files, temp_files = train_test_split(\n",
    "            files, test_size=0.3, random_state=42  # 30% for val+test\n",
    "        )\n",
    "        val_files, test_files = train_test_split(\n",
    "            temp_files, test_size=0.5, random_state=42  # Split 30% into 15% val + 15% test\n",
    "        )\n",
    "        \n",
    "        # Copy files\n",
    "        def copy_files(files, split):\n",
    "            src = os.path.join(SOURCE_DIR, cls)\n",
    "            dst = os.path.join(DEST_DIR, split, cls)\n",
    "            for f in files:\n",
    "                shutil.copy(os.path.join(src, f), dst)\n",
    "        \n",
    "        copy_files(train_files, 'train')\n",
    "        copy_files(val_files, 'val')\n",
    "        copy_files(test_files, 'test')\n",
    "\n",
    "def verify_splits():\n",
    "    \"\"\"Verify final distribution\"\"\"\n",
    "    print('\\nVerification:')\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f'\\n{split.upper()}:')\n",
    "        for cls in CLASSES:\n",
    "            path = os.path.join(DEST_DIR, split, cls)\n",
    "            count = len(os.listdir(path))\n",
    "            print(f'{cls}: {count} images')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_directory_structure()\n",
    "    split_and_copy()\n",
    "    verify_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18bda188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PATH VERIFICATION ===\n",
      "1. SOURCE_DIR exists? True\n",
      "2. Benign folder exists? False\n",
      "3. Malignant folder exists? False\n",
      "4. Full benign path: C:\\Users\\hatem\\codes\\breast cancer detection for course project\\pic\\benign\n",
      "5. Current working directory: c:\\Users\\hatem\\codes\\breast cancer detection for course project\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this right after the imports\n",
    "print(\"\\n=== PATH VERIFICATION ===\")\n",
    "print(\"1. SOURCE_DIR exists?\", os.path.exists(SOURCE_DIR))\n",
    "print(\"2. Benign folder exists?\", os.path.exists(os.path.join(SOURCE_DIR, 'benign')))\n",
    "print(\"3. Malignant folder exists?\", os.path.exists(os.path.join(SOURCE_DIR, 'malignant')))\n",
    "print(\"4. Full benign path:\", os.path.abspath(os.path.join(SOURCE_DIR, 'benign')))\n",
    "print(\"5. Current working directory:\", os.getcwd())  # Corrected line\n",
    "print(\"=========================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c9e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data\\train\\benign\n",
      "Created: data\\train\\malignant\n",
      "Created: data\\val\\benign\n",
      "Created: data\\val\\malignant\n",
      "Created: data\\test\\benign\n",
      "Created: data\\test\\malignant\n",
      "\n",
      "Total benign images: 4574\n",
      "benign split:\n",
      "- Train: 3201\n",
      "- Val: 686\n",
      "- Test: 687\n",
      "\n",
      "Total malignant images: 4442\n",
      "malignant split:\n",
      "- Train: 3109\n",
      "- Val: 666\n",
      "- Test: 667\n",
      "\n",
      "Final verification:\n",
      "train/benign: 3201\n",
      "train/malignant: 3109\n",
      "Total train: 6310\n",
      "\n",
      "val/benign: 686\n",
      "val/malignant: 666\n",
      "Total val: 1352\n",
      "\n",
      "test/benign: 687\n",
      "test/malignant: 667\n",
      "Total test: 1354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "SOURCE_DIR = 'pic'  # Contains train/ and val/\n",
    "DEST_DIR = 'data'   # Will create train/val/test here\n",
    "CLASSES = ['benign', 'malignant']\n",
    "SPLIT_RATIOS = (0.7, 0.15, 0.15)  # Train, Val, Test\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Create empty directory structure\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for cls in CLASSES:\n",
    "            path = os.path.join(DEST_DIR, split, cls)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(f'Created: {path}')\n",
    "\n",
    "def collect_and_split():\n",
    "    \"\"\"Collect from existing splits and create new test set\"\"\"\n",
    "    for cls in CLASSES:\n",
    "        # Collect all images from both train and val\n",
    "        all_images = []\n",
    "        \n",
    "        # From original train\n",
    "        train_src = os.path.join(SOURCE_DIR, 'train', cls)\n",
    "        all_images += [os.path.join(train_src, f) for f in os.listdir(train_src)]\n",
    "        \n",
    "        # From original val\n",
    "        val_src = os.path.join(SOURCE_DIR, 'val', cls)\n",
    "        all_images += [os.path.join(val_src, f) for f in os.listdir(val_src)]\n",
    "        \n",
    "        print(f'\\nTotal {cls} images:', len(all_images))\n",
    "        \n",
    "        # Split into new ratios (70% train, 15% val, 15% test)\n",
    "        train_files, temp_files = train_test_split(\n",
    "            all_images, \n",
    "            test_size=SPLIT_RATIOS[1] + SPLIT_RATIOS[2],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        val_files, test_files = train_test_split(\n",
    "            temp_files,\n",
    "            test_size=SPLIT_RATIOS[2]/(SPLIT_RATIOS[1] + SPLIT_RATIOS[2]),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Copy to new structure\n",
    "        def copy_files(files, split):\n",
    "            for src_path in files:\n",
    "                fname = os.path.basename(src_path)\n",
    "                dst_path = os.path.join(DEST_DIR, split, cls, fname)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "        \n",
    "        copy_files(train_files, 'train')\n",
    "        copy_files(val_files, 'val')\n",
    "        copy_files(test_files, 'test')\n",
    "        \n",
    "        print(f'{cls} split:')\n",
    "        print(f'- Train: {len(train_files)}')\n",
    "        print(f'- Val: {len(val_files)}')\n",
    "        print(f'- Test: {len(test_files)}')\n",
    "\n",
    "def verify_splits():\n",
    "    \"\"\"Verify final distribution\"\"\"\n",
    "    print('\\nFinal verification:')\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        total = 0\n",
    "        for cls in CLASSES:\n",
    "            count = len(os.listdir(os.path.join(DEST_DIR, split, cls)))\n",
    "            total += count\n",
    "            print(f'{split}/{cls}: {count}')\n",
    "        print(f'Total {split}: {total}\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_directory_structure()\n",
    "    collect_and_split()\n",
    "    verify_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2473464",
   "metadata": {},
   "source": [
    "Count the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Split    Benign    Malignant    Total\n",
      "-------  --------  -----------  -------\n",
      " TRAIN     3201       3109       6310\n",
      "  VAL      686         666       1352\n",
      " TEST      687         667       1354\n",
      "\n",
      "Class Balance Ratios:\n",
      "TRAIN: Benign:Malignant = 1.03:1\n",
      "VAL: Benign:Malignant = 1.03:1\n",
      "TEST: Benign:Malignant = 1.03:1\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "def count_images(base_path=\"data\"):\n",
    "    counts = {}\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        counts[split] = {}\n",
    "        for cls in [\"benign\", \"malignant\"]:\n",
    "            path = os.path.join(base_path, split, cls)\n",
    "            if os.path.exists(path):\n",
    "                num_images = len([f for f in os.listdir(path) \n",
    "                                if os.path.isfile(os.path.join(path, f))])\n",
    "                counts[split][cls] = num_images\n",
    "            else:\n",
    "                counts[split][cls] = 0\n",
    "                \n",
    "    # Create table\n",
    "    table = []\n",
    "    for split in counts:\n",
    "        row = [split.upper()]\n",
    "        row.append(counts[split][\"benign\"])\n",
    "        row.append(counts[split][\"malignant\"])\n",
    "        row.append(sum(counts[split].values()))\n",
    "        table.append(row)\n",
    "    \n",
    "    headers = [\"Split\", \"Benign\", \"Malignant\", \"Total\"]\n",
    "    print(tabulate(table, headers=headers, numalign=\"center\", stralign=\"center\"))\n",
    "    print(\"\\nClass Balance Ratios:\")\n",
    "    for split in counts:\n",
    "        total = sum(counts[split].values())\n",
    "        if total > 0:\n",
    "            ratio = counts[split][\"benign\"] / counts[split][\"malignant\"]\n",
    "            print(f\"{split.upper()}: Benign:Malignant = {ratio:.2f}:1\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ced51",
   "metadata": {},
   "source": [
    "Import the required liaberis to do deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96811670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c777baf",
   "metadata": {},
   "source": [
    "Assign the variables to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dd864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 150\n",
    "\n",
    "train_dir = r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\train'\n",
    "validation_dir = r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\val'\n",
    "test_dir = r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\data\\test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ecf8c",
   "metadata": {},
   "source": [
    "Generate more dato for the model and use VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bee2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6310 images belonging to 2 classes.\n",
      "Found 1352 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=28,\n",
    "                                   height_shift_range = 0.1,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   shear_range = 0.2 ,\n",
    "                                   horizontal_flip = True,\n",
    "                                   zoom_range = 0.3,\n",
    "                                   fill_mode = 'reflect',\n",
    "                                   brightness_range=[0.8,1.2]\n",
    "\n",
    "                                   )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode = 'binary'\n",
    "\n",
    ")\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False,input_shape=(image_size,\n",
    "                                                                     image_size,3))\n",
    "\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69478f7",
   "metadata": {},
   "source": [
    "Giving the model the way to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()    #Use sequential model in this model\n",
    "model2.add(conv_base) #Add the convoloution layer \n",
    "model2.add(layers.Flatten())#We must flatten the images before entering to model and the pics must be in ONE DIMENSION\n",
    "model2.add(layers.Dense(512))#We use 512 neural network because it is classification between malignant and benign \n",
    "model2.add(layers.Activation('relu'))#Relu is very good to make the data more complex\n",
    "model2.add(layers.Dropout(0.2))#dropout to aviod the overfit and it is in small amount\n",
    "model2.add(layers.Dense(1))#1 neural because it choose betweem two outputs only\n",
    "model2.add(layers.Activation('sigmoid'))#sigmoid is the best one here to select the higher probability between two \n",
    "\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\model.best.keras', monitor='val_loss',\n",
    "                               save_best_only=True)#Do checkpointer to save the best result model \n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0.001,patience=5,verbose=1,mode='max',\n",
    "restore_best_weights=True )#Do early stopping to reduce overfit\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])#Choosing adam for the optimizer and loss is binary bec. it classifies between two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2757d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 2s/step - accuracy: 0.5704 - loss: 0.6768 - val_accuracy: 0.8028 - val_loss: 0.5106\n",
      "Epoch 2/8\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2:18\u001b[0m 1s/step - accuracy: 0.7656 - loss: 0.5104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 368ms/step - accuracy: 0.7656 - loss: 0.5104 - val_accuracy: 0.7999 - val_loss: 0.5097\n",
      "Epoch 3/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.7388 - loss: 0.5442 - val_accuracy: 0.8229 - val_loss: 0.4546\n",
      "Epoch 4/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 340ms/step - accuracy: 0.7031 - loss: 0.5391 - val_accuracy: 0.8229 - val_loss: 0.4528\n",
      "Epoch 5/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.7766 - loss: 0.4864 - val_accuracy: 0.8363 - val_loss: 0.4220\n",
      "Epoch 6/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 325ms/step - accuracy: 0.8594 - loss: 0.4433 - val_accuracy: 0.8289 - val_loss: 0.4237\n",
      "Epoch 7/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 2s/step - accuracy: 0.7900 - loss: 0.4698 - val_accuracy: 0.8356 - val_loss: 0.4038\n",
      "Epoch 8/8\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 334ms/step - accuracy: 0.7500 - loss: 0.4531 - val_accuracy: 0.8363 - val_loss: 0.4023\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "for layer in conv_base.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2.compile(optimizer=optimizers.Adam(1e-5),loss='binary_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "history = model2.fit(train_generator,\n",
    "                     steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                     epochs=8,\n",
    "                     validation_data=validation_generator,\n",
    "                     validation_steps = validation_generator.samples//train_generator.batch_size,\n",
    "                     verbose =1,\n",
    "                     callbacks=[checkpointer,early])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591ec229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 1s/step - accuracy: 0.6977 - loss: 0.5665 - val_accuracy: 0.8207 - val_loss: 0.3977\n",
      "Epoch 2/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 2s/step - accuracy: 0.7977 - loss: 0.4325 - val_accuracy: 0.8713 - val_loss: 0.3308\n",
      "Epoch 3/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 2s/step - accuracy: 0.8405 - loss: 0.3683 - val_accuracy: 0.8743 - val_loss: 0.3102\n",
      "Epoch 4/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 2s/step - accuracy: 0.8455 - loss: 0.3479 - val_accuracy: 0.8839 - val_loss: 0.2947\n",
      "Epoch 5/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - accuracy: 0.8533 - loss: 0.3361 - val_accuracy: 0.8638 - val_loss: 0.3237\n",
      "Epoch 6/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.8558 - loss: 0.3373 - val_accuracy: 0.8847 - val_loss: 0.2886\n",
      "Epoch 7/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.8626 - loss: 0.3127 - val_accuracy: 0.8795 - val_loss: 0.2836\n",
      "Epoch 8/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.8612 - loss: 0.3160 - val_accuracy: 0.8996 - val_loss: 0.2471\n",
      "Epoch 9/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - accuracy: 0.8711 - loss: 0.3001 - val_accuracy: 0.8906 - val_loss: 0.2628\n",
      "Epoch 10/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 2s/step - accuracy: 0.8819 - loss: 0.2852 - val_accuracy: 0.8802 - val_loss: 0.2767\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(conv_base)\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(512))\n",
    "model2.add(layers.Activation('relu'))\n",
    "model2.add(layers.Dropout(0.2))\n",
    "model2.add(layers.Dense(1))\n",
    "model2.add(layers.Activation('sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=r'C:\\Users\\hatem\\codes\\breast cancer detection for course project\\model.best.keras', monitor='val_loss',\n",
    "                               save_best_only=True)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0.001,patience=5,verbose=1,mode='max',\n",
    "restore_best_weights=True )\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "history = model2.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = math.ceil(train_generator.samples / train_generator.batch_size)\n",
    ",\n",
    "    epochs = 10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = validation_generator.samples // validation_generator.batch_size,\n",
    "    callbacks=[checkpointer, early]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23537fa",
   "metadata": {},
   "source": [
    "90% TEST ACCUARCY!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c3ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1354 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9110 - loss: 0.2314\n",
      "test acc: 0.9055059552192688\n",
      "test loss 0.24942484498023987\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = 64,\n",
    "    class_mode = 'binary'\n",
    "\n",
    ")\n",
    "test_loss, test_acc = model2.evaluate(test_generator,steps=math.ceil(test_generator.samples//test_generator.batch_size))\n",
    "print('test acc:',test_acc)\n",
    "print('test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced62fa",
   "metadata": {},
   "source": [
    "I used the second one because it has better credentails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model2.save(\"breast_cancer_modelbest.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aae1c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
